{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab72dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yashl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yashl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yashl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yashl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yashl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yashl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comfortable</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happened to find it in a store, and i'm glad i did bc i never would have ordered it online bc it's petite.  i bought a petite and am 5'8\".  i love the length...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i co...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
       "0           0          767   33                      NaN   \n",
       "1           1         1080   34                      NaN   \n",
       "2           2         1077   60  Some major design flaws   \n",
       "3           3         1049   50         My favorite buy!   \n",
       "4           4          847   47         Flattering shirt   \n",
       "\n",
       "                                                                                                                                                                                               Review Text  \\\n",
       "0                                                                                                                                                    Absolutely wonderful - silky and sexy and comfortable   \n",
       "1  Love this dress!  it's sooo pretty.  i happened to find it in a store, and i'm glad i did bc i never would have ordered it online bc it's petite.  i bought a petite and am 5'8\".  i love the length...   \n",
       "2  I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i co...   \n",
       "3                                                                             I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!   \n",
       "4         This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!   \n",
       "\n",
       "   Rating  Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0       4                1                        0       Initmates   \n",
       "1       5                1                        4         General   \n",
       "2       3                0                        0         General   \n",
       "3       5                1                        0  General Petite   \n",
       "4       5                1                        6         General   \n",
       "\n",
       "  Department Name Class Name  \n",
       "0        Intimate  Intimates  \n",
       "1         Dresses    Dresses  \n",
       "2         Dresses    Dresses  \n",
       "3         Bottoms      Pants  \n",
       "4            Tops    Blouses  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud \n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, f1_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Importing plotly and cufflinks in offline mode\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "# Figure&Display options\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(10, 6)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)  # the size of A4 paper use (11.7, 8.27)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# !pip install termcolor\n",
    "import colorama\n",
    "from colorama import Fore, Style  # maakes strings colored\n",
    "from termcolor import colored\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\yashl\\OneDrive\\Documents\\womendress\\Womens Clothing E-Commerce Reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04466db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23486 reviews in this dataset\n",
      "Number of Duplicates: 21\n",
      "Number of Missing Values: 4697\n"
     ]
    }
   ],
   "source": [
    "# Remove columns\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "# How many reviews do we have?\n",
    "print('There are', df.shape[0], 'reviews in this dataset')\n",
    "\n",
    "# Do we have duplicates?\n",
    "print('Number of Duplicates:', len(df[df.duplicated()]))\n",
    "\n",
    "# Do we have missing values?\n",
    "print('Number of Missing Values:', df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c46504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Title                      3810\n",
       "Review Text                 845\n",
       "Division Name                14\n",
       "Department Name              14\n",
       "Class Name                   14\n",
       "Clothing ID                   0\n",
       "Age                           0\n",
       "Rating                        0\n",
       "Recommended IND               0\n",
       "Positive Feedback Count       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of Missing Values per column:')\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1dae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with nulls in specific columns\n",
    "df = df.dropna(subset = ['Review Text', 'Division Name', 'Department Name', 'Class Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f32f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0b3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just work with the reviews and recommendations\n",
    "data = df[['Review Text', 'Recommended IND']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926e634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, \n",
    "    remove links, remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79e642bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Review Text'] = data['Review Text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec687049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the evaluation metrics\n",
    "def roc_auc(predictions,target):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "df1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1813634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split target & features\n",
    "# recommend IND : 1 == would recommend, 0 == not recommend\n",
    "X = data.drop('Recommended IND', axis=1)\n",
    "y = data['Recommended IND']\n",
    "\n",
    "# Spliting train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48adf7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded training shape: (18102, 112)\n",
      "\n",
      "Padded test shape: (4526, 112)\n"
     ]
    }
   ],
   "source": [
    "# Keras takenization text data prep\n",
    "\n",
    "num_words = None   # the most X frequent words is returned\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train['Review Text'].tolist() + X_test['Review Text'].tolist())   # introduce text in list\n",
    "\n",
    "# Get data word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Encode training/test data sentences into sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train['Review Text'].tolist())\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test['Review Text'].tolist())\n",
    "df_test_seq = tokenizer.texts_to_sequences(df1['Review Text'].tolist())\n",
    "\n",
    "# Get max training sequence length\n",
    "max_len = max([len(x) for x in X_train_seq])\n",
    "\n",
    "# Pad the training/test sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "df_pad = pad_sequences(df_test_seq, maxlen=max_len)\n",
    "\n",
    "# Output some results \n",
    "print(\"\\nPadded training shape:\", X_train_pad.shape)\n",
    "print(\"\\nPadded test shape:\", X_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ee3f0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 112, 50)           1022500   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               60400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,083,001\n",
      "Trainable params: 1,083,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    50,     # embeds it in a 50-dimensional vector\n",
    "                    input_length=max_len))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57f7cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "36/36 [==============================] - 46s 1s/step - loss: 0.5152 - accuracy: 0.8130\n",
      "Epoch 2/6\n",
      "36/36 [==============================] - 44s 1s/step - loss: 0.3863 - accuracy: 0.8273\n",
      "Epoch 3/6\n",
      "36/36 [==============================] - 35s 984ms/step - loss: 0.2648 - accuracy: 0.8874\n",
      "Epoch 4/6\n",
      "36/36 [==============================] - 34s 951ms/step - loss: 0.2077 - accuracy: 0.9129\n",
      "Epoch 5/6\n",
      "36/36 [==============================] - 35s 965ms/step - loss: 0.1750 - accuracy: 0.9282\n",
      "Epoch 6/6\n",
      "36/36 [==============================] - 35s 969ms/step - loss: 0.1590 - accuracy: 0.9377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2df089c79d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "model.fit(X_train_pad, y_train, epochs=6, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ea30431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 3s 17ms/step\n",
      "AUC: 0.92%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(X_test_pad)\n",
    "print(\"AUC: %.2f%%\" % (roc_auc(scores,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba465256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sklearn classes \n",
    "from sklearn.model_selection import train_test_split,RepeatedKFold, cross_val_score,KFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# sklearn utility to compare algorithms\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Visualisation Libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2999819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e211259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Review Text\n",
      "7387   love this and wish it came in more colors the ...\n",
      "8791   i am  and  lbs and bought a size s it fits but...\n",
      "12191  i ordered  different styles of jogger pants an...\n",
      "4023   i had been eyeing the maroon one and i snatche...\n",
      "7897   i ordered the size xl and it fits me as it doe...\n",
      "...                                                  ...\n",
      "21381  i just received this blouse and love itmore be...\n",
      "5982   this top is beautiful and fits true to size   ...\n",
      "15395  great dress nicely structured but the fabric i...\n",
      "12264  i generally wear a  jeans fit perfectly skinny...\n",
      "13178  i just purchased these jeans online in my regu...\n",
      "\n",
      "[18102 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de2d0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_count = vectorizer.fit_transform(X_train['Review Text'])\n",
    "X_test_count = vectorizer.transform(X_test['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d81fc771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8253)\t1\n",
      "  (0, 14682)\t1\n",
      "  (0, 434)\t5\n",
      "  (0, 16399)\t1\n",
      "  (0, 7230)\t3\n",
      "  (0, 2076)\t1\n",
      "  (0, 6940)\t3\n",
      "  (0, 8993)\t1\n",
      "  (0, 2729)\t1\n",
      "  (0, 14546)\t3\n",
      "  (0, 4905)\t1\n",
      "  (0, 3432)\t1\n",
      "  (0, 616)\t1\n",
      "  (0, 13281)\t2\n",
      "  (0, 2797)\t1\n",
      "  (0, 1652)\t1\n",
      "  (0, 9139)\t1\n",
      "  (0, 15670)\t1\n",
      "  (0, 12898)\t1\n",
      "  (0, 15987)\t1\n",
      "  (0, 8077)\t1\n",
      "  (0, 12070)\t1\n",
      "  (0, 14895)\t3\n",
      "  (0, 12340)\t1\n",
      "  (0, 14537)\t1\n",
      "  :\t:\n",
      "  (18101, 14555)\t1\n",
      "  (18101, 12039)\t1\n",
      "  (18101, 10387)\t1\n",
      "  (18101, 7369)\t3\n",
      "  (18101, 5100)\t1\n",
      "  (18101, 12939)\t1\n",
      "  (18101, 5603)\t1\n",
      "  (18101, 16074)\t1\n",
      "  (18101, 11538)\t1\n",
      "  (18101, 346)\t1\n",
      "  (18101, 5918)\t1\n",
      "  (18101, 617)\t1\n",
      "  (18101, 9319)\t1\n",
      "  (18101, 12490)\t1\n",
      "  (18101, 9820)\t1\n",
      "  (18101, 14545)\t1\n",
      "  (18101, 10878)\t1\n",
      "  (18101, 14807)\t1\n",
      "  (18101, 866)\t1\n",
      "  (18101, 9316)\t1\n",
      "  (18101, 14005)\t1\n",
      "  (18101, 4573)\t1\n",
      "  (18101, 14629)\t1\n",
      "  (18101, 10360)\t1\n",
      "  (18101, 4781)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cfe20272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e41c47be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaaaannnnnnd',\n",
       " 'aaaahs',\n",
       " 'aaahed',\n",
       " 'aame',\n",
       " 'ab',\n",
       " 'abbey',\n",
       " 'abby',\n",
       " 'abdomen',\n",
       " 'abercrombie',\n",
       " 'abhor',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'able',\n",
       " 'abnormally',\n",
       " 'abo',\n",
       " 'abolutely',\n",
       " 'abou',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abovetheknee',\n",
       " 'abroad',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'abso',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutley',\n",
       " 'absolutly',\n",
       " 'absoluty',\n",
       " 'absorbent',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdly',\n",
       " 'abt',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abused',\n",
       " 'abut',\n",
       " 'ac',\n",
       " 'acacia',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accenting',\n",
       " 'accents',\n",
       " 'accentuate',\n",
       " 'accentuatea',\n",
       " 'accentuated',\n",
       " 'accentuates',\n",
       " 'accentuating',\n",
       " 'accept',\n",
       " 'acceptabl',\n",
       " 'acceptable',\n",
       " 'acceptably',\n",
       " 'accepted',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessories',\n",
       " 'accessoriesanticipating',\n",
       " 'accessoriesi',\n",
       " 'accessorize',\n",
       " 'accessorized',\n",
       " 'accessorizes',\n",
       " 'accessorizing',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'acco',\n",
       " 'accomadate',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodates',\n",
       " 'accommodating',\n",
       " 'accomodate',\n",
       " 'accompanies',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accordian',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accumulate',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accuratei',\n",
       " 'accurately',\n",
       " 'ace',\n",
       " 'acetate',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'ack',\n",
       " 'acknowledge',\n",
       " 'acomfortable',\n",
       " 'acquaintance',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'across',\n",
       " 'acrossed',\n",
       " 'acrylic',\n",
       " 'acrylicfeeling',\n",
       " 'act',\n",
       " 'action',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activewear',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actuallly',\n",
       " 'actually',\n",
       " 'actualyl',\n",
       " 'acup',\n",
       " 'acutually',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'additio',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additonal',\n",
       " 'addon',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'adds',\n",
       " 'addtl',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhesive',\n",
       " 'adieu',\n",
       " 'adirondacks',\n",
       " 'adj',\n",
       " 'adjacent',\n",
       " 'adjsut',\n",
       " 'adjust',\n",
       " 'adjustability',\n",
       " 'adjustable',\n",
       " 'adjustableand',\n",
       " 'adjusted',\n",
       " 'adjustedthen',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'admirable',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admirin',\n",
       " 'admiring',\n",
       " 'admit',\n",
       " 'admittedly',\n",
       " 'adn',\n",
       " 'adopted',\n",
       " 'ador',\n",
       " 'adorable',\n",
       " 'adorableso',\n",
       " 'adorably',\n",
       " 'adore',\n",
       " 'adoreable',\n",
       " 'adored',\n",
       " 'adores',\n",
       " 'adoring',\n",
       " 'adorn',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'adventure',\n",
       " 'adventurous',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisementas',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advises',\n",
       " 'aesthetic',\n",
       " 'aesthetically',\n",
       " 'aestheticdesign',\n",
       " 'aesthetics',\n",
       " 'aestheticsweet',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affects',\n",
       " 'affixed',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afforded',\n",
       " 'affords',\n",
       " 'aflutter',\n",
       " 'aforementioned',\n",
       " 'afraid',\n",
       " 'aframe',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'afterwards',\n",
       " 'afterwork',\n",
       " 'ag',\n",
       " 'aga',\n",
       " 'agai',\n",
       " 'again',\n",
       " 'againi',\n",
       " 'againits',\n",
       " 'againlove',\n",
       " 'against',\n",
       " 'againuntil',\n",
       " 'againvery',\n",
       " 'age',\n",
       " 'ageappropriate',\n",
       " 'aged',\n",
       " 'ageing',\n",
       " 'ageless',\n",
       " 'ages',\n",
       " 'aggressive',\n",
       " 'aghast',\n",
       " 'agin',\n",
       " 'agitated',\n",
       " 'agitates',\n",
       " 'ago',\n",
       " 'agoand',\n",
       " 'agoi',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'ags',\n",
       " 'agustable',\n",
       " 'ah',\n",
       " 'ahd',\n",
       " 'ahe',\n",
       " 'ahead',\n",
       " 'aheadpress',\n",
       " 'ahge',\n",
       " 'ahh',\n",
       " 'ahold',\n",
       " 'aholed',\n",
       " 'ahs',\n",
       " 'ahtro',\n",
       " 'ahve',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aided',\n",
       " 'aiming',\n",
       " 'air',\n",
       " 'airconditioned',\n",
       " 'airconditionied',\n",
       " 'airdried',\n",
       " 'airdrying',\n",
       " 'airflow',\n",
       " 'airier',\n",
       " 'airiness',\n",
       " 'airing',\n",
       " 'airline',\n",
       " 'airly',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airy',\n",
       " 'airynot',\n",
       " 'airyvery',\n",
       " 'aize',\n",
       " 'aka',\n",
       " 'akemi',\n",
       " 'akemikin',\n",
       " 'akin',\n",
       " 'al',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'albeit',\n",
       " 'alce',\n",
       " 'aleardy',\n",
       " 'alerations',\n",
       " 'alert',\n",
       " 'alexa',\n",
       " 'alexandria',\n",
       " 'align',\n",
       " 'aligned',\n",
       " 'aligning',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'aline',\n",
       " 'alined',\n",
       " 'alinefitandflare',\n",
       " 'alinefitnflare',\n",
       " 'alines',\n",
       " 'alittle',\n",
       " 'all',\n",
       " 'allalthough',\n",
       " 'allaround',\n",
       " 'allbut',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'alleviate',\n",
       " 'alleviated',\n",
       " 'alley',\n",
       " 'allfyi',\n",
       " 'allin',\n",
       " 'allinall',\n",
       " 'allison',\n",
       " 'allit',\n",
       " 'alllll',\n",
       " 'allllllmost',\n",
       " 'allone',\n",
       " 'allot',\n",
       " 'allover',\n",
       " 'allovertheplace',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allpolyester',\n",
       " 'allpurpose',\n",
       " 'allround',\n",
       " 'allseason',\n",
       " 'allsummertime',\n",
       " 'allthe',\n",
       " 'allthis',\n",
       " 'alludes',\n",
       " 'alluring',\n",
       " 'alluringnot',\n",
       " 'allusion',\n",
       " 'allusione',\n",
       " 'allways',\n",
       " 'allweather',\n",
       " 'allyear',\n",
       " 'almost',\n",
       " 'almostcowl',\n",
       " 'almostlingerielike',\n",
       " 'almsot',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alpaca',\n",
       " 'alpacawool',\n",
       " 'alr',\n",
       " 'alrea',\n",
       " 'alread',\n",
       " 'already',\n",
       " 'alreadyi',\n",
       " 'alreadyit',\n",
       " 'alright',\n",
       " 'als',\n",
       " 'also',\n",
       " 'alsoi',\n",
       " 'alsonot',\n",
       " 'alsothe',\n",
       " 'alst',\n",
       " 'alt',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alteration',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'alteredbut',\n",
       " 'alteredi',\n",
       " 'alteredif',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternations',\n",
       " 'alternative',\n",
       " 'althetic',\n",
       " 'althleisure',\n",
       " 'altho',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'altogether',\n",
       " 'alway',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amadi',\n",
       " 'amalfi',\n",
       " 'amaz',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazedi',\n",
       " 'amazement',\n",
       " 'amazing',\n",
       " 'amazingi',\n",
       " 'amazingit',\n",
       " 'amazingly',\n",
       " 'amazingmuch',\n",
       " 'amazingsuch',\n",
       " 'amazingsuper',\n",
       " 'amazingworth',\n",
       " 'amazon',\n",
       " 'amber',\n",
       " 'ambiguous',\n",
       " 'ambitious',\n",
       " 'amcant',\n",
       " 'amd',\n",
       " 'amelia',\n",
       " 'amenable',\n",
       " 'american',\n",
       " 'amidst',\n",
       " 'among',\n",
       " 'amorphous',\n",
       " 'amost',\n",
       " 'amount',\n",
       " 'amountbut',\n",
       " 'amounts',\n",
       " 'ampampamp',\n",
       " 'ample',\n",
       " 'amply',\n",
       " 'amputated',\n",
       " 'amsterdam',\n",
       " 'amt',\n",
       " 'amterial',\n",
       " 'an',\n",
       " 'anafa',\n",
       " 'analogy',\n",
       " 'analysis',\n",
       " 'anatomy',\n",
       " 'anchors',\n",
       " 'and',\n",
       " 'andflattering',\n",
       " 'andheavy',\n",
       " 'andhonestlythey',\n",
       " 'andnonary',\n",
       " 'andor',\n",
       " 'andrews',\n",
       " 'ands',\n",
       " 'andunrolledthey',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angle',\n",
       " 'angled',\n",
       " 'angles',\n",
       " 'angling',\n",
       " 'angora',\n",
       " 'anhtro',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'anita',\n",
       " 'ankl',\n",
       " 'ankle',\n",
       " 'anklelength',\n",
       " 'ankles',\n",
       " 'anklesthe',\n",
       " 'anlobn',\n",
       " 'anna',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'anniversaryi',\n",
       " 'annnnd',\n",
       " 'announce',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoys',\n",
       " 'anns',\n",
       " 'annual',\n",
       " 'anomaly',\n",
       " 'anorak',\n",
       " 'anorexic',\n",
       " 'anoth',\n",
       " 'another',\n",
       " 'anotherbeware',\n",
       " 'anothr',\n",
       " 'anrtho',\n",
       " 'ans',\n",
       " 'answe',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'ante',\n",
       " 'anth',\n",
       " 'anther',\n",
       " 'antheropologie',\n",
       " 'anthletic',\n",
       " 'antho',\n",
       " 'anthopology',\n",
       " 'anthto',\n",
       " 'anthtos',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anticipation',\n",
       " 'anticpated',\n",
       " 'antipant',\n",
       " 'antique',\n",
       " 'antistatic',\n",
       " 'antrho',\n",
       " 'antrhopologie',\n",
       " 'antro',\n",
       " 'antropologie',\n",
       " 'antropologies',\n",
       " 'antwerp',\n",
       " 'anwen',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anymoreim',\n",
       " 'anymoreso',\n",
       " 'anymorethe',\n",
       " 'anyone',\n",
       " 'anyonea',\n",
       " 'anyones',\n",
       " 'anyou',\n",
       " 'anything',\n",
       " 'anythingjust',\n",
       " 'anythingthe',\n",
       " 'anythingtights',\n",
       " 'anytime',\n",
       " 'anytimeanywhere',\n",
       " 'anyw',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywayseven',\n",
       " 'anywhere',\n",
       " 'anywhereeven',\n",
       " 'anywhereeverywhere',\n",
       " 'anywhereits',\n",
       " 'anywherethis',\n",
       " 'anywhereyet',\n",
       " 'aottern',\n",
       " 'apart',\n",
       " 'apex',\n",
       " 'aplique',\n",
       " 'apocolypse',\n",
       " 'app',\n",
       " 'appalled',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'apparentlike',\n",
       " 'apparently',\n",
       " 'apparentlyperfect',\n",
       " 'appeal',\n",
       " 'appealed',\n",
       " 'appealing',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearanceim',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appearsperfect',\n",
       " 'appearsvery',\n",
       " 'appetite',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'appleish',\n",
       " 'appleshaped',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'applique',\n",
       " 'appliqueslaces',\n",
       " 'appliqu',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appopriate',\n",
       " 'appr',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'apprehension',\n",
       " 'apprehensive',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropirate',\n",
       " 'approprialty',\n",
       " 'appropriarely',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'appropriateness',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'approximate',\n",
       " 'approximately',\n",
       " 'apprx',\n",
       " 'apre',\n",
       " 'apricot',\n",
       " 'april',\n",
       " 'apron',\n",
       " 'aprts',\n",
       " 'apr',\n",
       " 'aps',\n",
       " 'apt',\n",
       " 'aptly',\n",
       " 'apttenrs',\n",
       " 'apttern',\n",
       " 'aqua',\n",
       " 'aquaflora',\n",
       " 'ar',\n",
       " 'arbor',\n",
       " 'arc',\n",
       " 'arched',\n",
       " 'architectural',\n",
       " 'arctic',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areaalso',\n",
       " 'areacould',\n",
       " 'areaeven',\n",
       " 'areaim',\n",
       " 'areameant',\n",
       " 'areanear',\n",
       " 'areas',\n",
       " 'areaso',\n",
       " 'areathis',\n",
       " 'areawin',\n",
       " 'arei',\n",
       " 'areif',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'arepros',\n",
       " 'arethe',\n",
       " 'arethey',\n",
       " 'areturn',\n",
       " 'arguably',\n",
       " 'argument',\n",
       " 'arias',\n",
       " 'arise',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'armarmpits',\n",
       " 'armchest',\n",
       " 'armed',\n",
       " 'armful',\n",
       " 'armh',\n",
       " 'armhoes',\n",
       " 'armhole',\n",
       " 'armholes',\n",
       " 'armholesbust',\n",
       " 'armholesthey',\n",
       " 'armneck',\n",
       " 'armpit',\n",
       " 'armpits',\n",
       " 'armpitslike',\n",
       " 'arms',\n",
       " 'armsand',\n",
       " 'armsbody',\n",
       " 'armsbut',\n",
       " 'armschest',\n",
       " 'armsevery',\n",
       " 'armsfell',\n",
       " 'armshit',\n",
       " 'armshoulder',\n",
       " 'armsif',\n",
       " 'armsshoulders',\n",
       " 'armsshouldersback',\n",
       " 'armsshouldersbust',\n",
       " 'armstorso',\n",
       " 'armsyoull',\n",
       " 'armwise',\n",
       " 'armwould',\n",
       " 'army',\n",
       " 'aro',\n",
       " 'arose',\n",
       " 'aroun',\n",
       " 'around',\n",
       " 'aroundholds',\n",
       " 'aroundnope',\n",
       " 'aroundstylefit',\n",
       " 'arr',\n",
       " 'arranged',\n",
       " 'arrangement',\n",
       " 'arrangements',\n",
       " 'array',\n",
       " 'arrival',\n",
       " 'arrivals',\n",
       " 'arrive',\n",
       " 'arriveand',\n",
       " 'arrived',\n",
       " 'arrivedlove',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrow',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'artfully',\n",
       " 'article',\n",
       " 'articlepattern',\n",
       " 'articles',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artistically',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'artsy',\n",
       " 'artwork',\n",
       " 'arty',\n",
       " 'artylooking',\n",
       " 'as',\n",
       " 'asadvertised',\n",
       " 'asap',\n",
       " 'ascetics',\n",
       " 'asflattering',\n",
       " 'ashaped',\n",
       " 'asheville',\n",
       " 'ashley',\n",
       " 'asia',\n",
       " 'asianinspired',\n",
       " 'aside',\n",
       " 'asis',\n",
       " 'ask',\n",
       " 'aske',\n",
       " 'asked',\n",
       " 'askedher',\n",
       " 'asking',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'aspictured',\n",
       " 'asscoaites',\n",
       " 'assemblage',\n",
       " 'assembledcomfortable',\n",
       " 'assessed',\n",
       " 'assessing',\n",
       " 'assessment',\n",
       " 'assessments',\n",
       " 'assets',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assistantstyle',\n",
       " 'assoc',\n",
       " 'associate',\n",
       " 'association',\n",
       " 'assorted',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'asters',\n",
       " 'asthetic',\n",
       " 'astoria',\n",
       " 'astounded',\n",
       " 'asummig',\n",
       " 'aswing',\n",
       " 'asylum',\n",
       " 'asymetrical',\n",
       " 'asymmetric',\n",
       " 'asymmetrical',\n",
       " 'asymmetry',\n",
       " 'at',\n",
       " 'atbarely',\n",
       " 'ate',\n",
       " 'athleisure',\n",
       " 'athlete',\n",
       " 'athletic',\n",
       " 'athletically',\n",
       " 'athleticbuilt',\n",
       " 'athletichourglass',\n",
       " 'athleticism',\n",
       " 'athleticmuscular',\n",
       " 'athleticslim',\n",
       " 'athleticstraight',\n",
       " 'athleticwear',\n",
       " 'athough',\n",
       " 'athro',\n",
       " 'atknee',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atleast',\n",
       " 'atrocious',\n",
       " 'atrociously',\n",
       " 'attach',\n",
       " 'attached',\n",
       " 'attaches',\n",
       " 'attaching',\n",
       " 'attachment',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attempts',\n",
       " 'attend',\n",
       " 'attendant',\n",
       " 'attendants',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attent',\n",
       " 'attentio',\n",
       " 'attention',\n",
       " 'attentionthe',\n",
       " 'attest',\n",
       " 'attic',\n",
       " 'attire',\n",
       " 'attitude',\n",
       " 'attn',\n",
       " 'attrac',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'attractively',\n",
       " 'attracts',\n",
       " 'attributed',\n",
       " 'attributes',\n",
       " 'atypical',\n",
       " 'aubergine',\n",
       " 'auction',\n",
       " 'audience',\n",
       " 'audrey',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'austin',\n",
       " 'australian',\n",
       " 'authentic',\n",
       " 'authenticity',\n",
       " 'autobots',\n",
       " 'automatically',\n",
       " 'autumn',\n",
       " 'autumnal',\n",
       " 'autumnearly',\n",
       " 'autumnwinter',\n",
       " 'av',\n",
       " 'avai',\n",
       " 'avail',\n",
       " 'availab',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'availalbe',\n",
       " 'ave',\n",
       " 'average',\n",
       " 'averaged',\n",
       " 'averagethe',\n",
       " 'avergage',\n",
       " 'aversion',\n",
       " 'averting',\n",
       " 'avery',\n",
       " 'avg',\n",
       " 'avid',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'avoids',\n",
       " 'aw',\n",
       " 'awaited',\n",
       " 'awaiting',\n",
       " 'awakward',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'awate',\n",
       " 'away',\n",
       " 'awaysecond',\n",
       " 'awe',\n",
       " 'aweful',\n",
       " 'awesome',\n",
       " 'awesomealthough',\n",
       " 'awesomefrom',\n",
       " 'awesomely',\n",
       " 'awful',\n",
       " 'awfuli',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awhilesnatched',\n",
       " 'awkward',\n",
       " 'awkwardbunchy',\n",
       " 'awkwardfitting',\n",
       " 'awkwardly',\n",
       " 'awkwardlyannoyingly',\n",
       " 'awkwardlystrangely',\n",
       " 'awkwards',\n",
       " 'axtra',\n",
       " 'az',\n",
       " 'ba',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babybridal',\n",
       " 'babydoll',\n",
       " 'babys',\n",
       " 'bac',\n",
       " 'bachelorette',\n",
       " 'back',\n",
       " 'backarms',\n",
       " 'backbut',\n",
       " 'backchest',\n",
       " 'backdear',\n",
       " 'backdesign',\n",
       " 'backed',\n",
       " 'backend',\n",
       " 'backfront',\n",
       " 'background',\n",
       " 'backi',\n",
       " 'backit',\n",
       " 'backjust',\n",
       " 'backless',\n",
       " 'backlike',\n",
       " 'backmodel',\n",
       " 'backnot',\n",
       " 'backorder',\n",
       " 'backordered',\n",
       " 'backpack',\n",
       " 'backpacker',\n",
       " 'backs',\n",
       " 'backshoulders',\n",
       " 'backshouldersupper',\n",
       " 'backside',\n",
       " 'backthe',\n",
       " 'backthick',\n",
       " 'backtoback',\n",
       " 'backup',\n",
       " 'backvback',\n",
       " 'backwaist',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'backwide',\n",
       " 'backwould',\n",
       " 'backyard',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badand',\n",
       " 'badass',\n",
       " 'badi',\n",
       " 'badis',\n",
       " 'badly',\n",
       " 'badwhen',\n",
       " 'badwornold',\n",
       " 'baffled',\n",
       " 'bag',\n",
       " 'bagg',\n",
       " 'baggage',\n",
       " 'bagged',\n",
       " 'bagger',\n",
       " 'baggie',\n",
       " 'baggier',\n",
       " 'bagginess',\n",
       " 'bagging',\n",
       " 'baggish',\n",
       " 'baggy',\n",
       " 'baggyall',\n",
       " 'baggybig',\n",
       " 'baggybut',\n",
       " 'baggyloose',\n",
       " 'baggylose',\n",
       " 'baggyreally',\n",
       " 'baggysloppy',\n",
       " 'baglike',\n",
       " 'bags',\n",
       " 'bahama',\n",
       " 'bahamas',\n",
       " 'bailey',\n",
       " 'bailley',\n",
       " 'bailly',\n",
       " 'baily',\n",
       " 'bainbridge',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balances',\n",
       " 'balancing',\n",
       " 'balc',\n",
       " 'balck',\n",
       " 'balked',\n",
       " 'ball',\n",
       " 'balled',\n",
       " 'ballerina',\n",
       " 'ballet',\n",
       " 'ballgown',\n",
       " 'balling',\n",
       " 'balloon',\n",
       " 'ballooned',\n",
       " 'balloonfit',\n",
       " 'ballooning',\n",
       " 'balloons',\n",
       " 'balloony',\n",
       " 'balls',\n",
       " 'balmy',\n",
       " 'baltimoreand',\n",
       " 'bam',\n",
       " 'bamboo',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bandages',\n",
       " 'bandaidcolored',\n",
       " 'bandeau',\n",
       " 'banded',\n",
       " ...]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "355136be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaannnnnnd</th>\n",
       "      <th>aaaahs</th>\n",
       "      <th>aaahed</th>\n",
       "      <th>aame</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abby</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abercrombie</th>\n",
       "      <th>...</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zippie</th>\n",
       "      <th>zipping</th>\n",
       "      <th>zips</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolanders</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>ber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18097</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18098</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18099</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18102 rows  16741 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaaaannnnnnd  aaaahs  aaahed  aame  ab  abbey  abby  abdomen  \\\n",
       "0       0             0       0       0     0   0      0     0        0   \n",
       "1       0             0       0       0     0   0      0     0        0   \n",
       "2       0             0       0       0     0   0      0     0        0   \n",
       "3       0             0       0       0     0   0      0     0        0   \n",
       "4       0             0       0       0     0   0      0     0        0   \n",
       "...    ..           ...     ...     ...   ...  ..    ...   ...      ...   \n",
       "18097   0             0       0       0     0   0      0     0        0   \n",
       "18098   0             0       0       0     0   0      0     0        0   \n",
       "18099   0             0       0       0     0   0      0     0        0   \n",
       "18100   0             0       0       0     0   0      0     0        0   \n",
       "18101   0             0       0       0     0   0      0     0        0   \n",
       "\n",
       "       abercrombie  ...  zippers  zippie  zipping  zips  zombie  zone  \\\n",
       "0                0  ...        0       0        0     0       0     0   \n",
       "1                0  ...        0       0        0     0       0     0   \n",
       "2                0  ...        0       0        0     0       0     0   \n",
       "3                0  ...        0       0        0     0       0     0   \n",
       "4                0  ...        0       0        0     0       0     0   \n",
       "...            ...  ...      ...     ...      ...   ...     ...   ...   \n",
       "18097            0  ...        0       0        0     0       0     0   \n",
       "18098            0  ...        0       0        0     0       0     0   \n",
       "18099            0  ...        0       0        0     0       0     0   \n",
       "18100            0  ...        0       0        0     0       0     0   \n",
       "18101            0  ...        0       0        0     0       0     0   \n",
       "\n",
       "       zoolanders  zoom  zooming  ber  \n",
       "0               0     0        0      0  \n",
       "1               0     0        0      0  \n",
       "2               0     0        0      0  \n",
       "3               0     0        0      0  \n",
       "4               0     0        0      0  \n",
       "...           ...   ...      ...    ...  \n",
       "18097           0     0        0      0  \n",
       "18098           0     0        0      0  \n",
       "18099           0     0        0      0  \n",
       "18100           0     0        0      0  \n",
       "18101           0     0        0      0  \n",
       "\n",
       "[18102 rows x 16741 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a4c8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd2e217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train['Review Text'])\n",
    "X_test_tf_idf = tf_idf_vectorizer.transform(X_test['Review Text'])\n",
    "df_test_tf_idf = tf_idf_vectorizer.transform(df1['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dee97e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf.toarray()\n",
    "df_test_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e3e6122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaannnnnnd</th>\n",
       "      <th>aaaahs</th>\n",
       "      <th>aaahed</th>\n",
       "      <th>aame</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abby</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abercrombie</th>\n",
       "      <th>...</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zippie</th>\n",
       "      <th>zipping</th>\n",
       "      <th>zips</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolanders</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>ber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18097</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18098</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18099</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18102 rows  16741 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aaaaannnnnnd  aaaahs  aaahed  aame   ab  abbey  abby  abdomen  \\\n",
       "0      0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "1      0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "2      0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "3      0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "4      0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "...    ...           ...     ...     ...   ...  ...    ...   ...      ...   \n",
       "18097  0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "18098  0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "18099  0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "18100  0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "18101  0.0           0.0     0.0     0.0   0.0  0.0    0.0   0.0      0.0   \n",
       "\n",
       "       abercrombie  ...  zippers  zippie  zipping  zips  zombie  zone  \\\n",
       "0              0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "1              0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "2              0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "3              0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "4              0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "...            ...  ...      ...     ...      ...   ...     ...   ...   \n",
       "18097          0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "18098          0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "18099          0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "18100          0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "18101          0.0  ...      0.0     0.0      0.0   0.0     0.0   0.0   \n",
       "\n",
       "       zoolanders  zoom  zooming  ber  \n",
       "0             0.0   0.0      0.0    0.0  \n",
       "1             0.0   0.0      0.0    0.0  \n",
       "2             0.0   0.0      0.0    0.0  \n",
       "3             0.0   0.0      0.0    0.0  \n",
       "4             0.0   0.0      0.0    0.0  \n",
       "...           ...   ...      ...    ...  \n",
       "18097         0.0   0.0      0.0    0.0  \n",
       "18098         0.0   0.0      0.0    0.0  \n",
       "18099         0.0   0.0      0.0    0.0  \n",
       "18100         0.0   0.0      0.0    0.0  \n",
       "18101         0.0   0.0      0.0    0.0  \n",
       "\n",
       "[18102 rows x 16741 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "92adf88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report, f1_score, recall_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c731282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_train, X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    print(\"Test_Set\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Train_Set\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    plot_confusion_matrix(model, X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd0471e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000,\n",
       "                   random_state=101)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logestic Regression\n",
    "log = LogisticRegression(C=0.1, max_iter=1000, random_state=101, class_weight=\"balanced\")\n",
    "\n",
    "log.fit(X_train_tf_idf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "493a5b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG MODEL\n",
      "Test_Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.87      0.67       820\n",
      "           1       0.97      0.84      0.90      3706\n",
      "\n",
      "    accuracy                           0.84      4526\n",
      "   macro avg       0.75      0.85      0.78      4526\n",
      "weighted avg       0.89      0.84      0.85      4526\n",
      "\n",
      "Train_Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.89      0.70      3281\n",
      "           1       0.97      0.85      0.91     14821\n",
      "\n",
      "    accuracy                           0.86     18102\n",
      "   macro avg       0.77      0.87      0.80     18102\n",
      "weighted avg       0.90      0.86      0.87     18102\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6448\\2682039904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LOG MODEL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_tf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_tf_idf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6448\\481625198.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(model, X_train, X_test)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"LOG MODEL\")\n",
    "\n",
    "eval(log, X_train_tf_idf, X_test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2874b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy             score for count : 0.8761465690028098\n",
      "\n",
      " precision-0          score for count : 0.6263786697768821\n",
      "\n",
      " recall-0             score for count : 0.7848218919119283\n",
      "\n",
      " f1-0                 score for count : 0.6964355181887051\n",
      "\n",
      " precision-1          score for count : 0.9496221131768099\n",
      "\n",
      " recall-1             score for count : 0.8963635097911281\n",
      "\n",
      " f1-1                 score for count : 0.922191001980688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "custom_scorer = {'accuracy': make_scorer(accuracy_score),\n",
    "                 'precision-0': make_scorer(precision_score, pos_label=0),\n",
    "                 'recall-0': make_scorer(recall_score, pos_label=0),\n",
    "                 'f1-0': make_scorer(f1_score, pos_label=0),\n",
    "                 'precision-1': make_scorer(precision_score, pos_label=1),\n",
    "                 'recall-1': make_scorer(recall_score, pos_label=1),\n",
    "                 'f1-1': make_scorer(f1_score, pos_label=1)\n",
    "                 }\n",
    "\n",
    "for i, j in custom_scorer.items():\n",
    "    model = LogisticRegression(C =0.6, max_iter=1000, class_weight= \"balanced\", random_state=101)\n",
    "    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n",
    "    if i == \"recall-1\":\n",
    "        log_count_rec = scores\n",
    "    elif i == \"f1-1\":\n",
    "        log_count_f1 = scores\n",
    "    print(f\" {i:20} score for count : {scores}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04e6cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tf_idf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0cb8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB MODEL\n",
      "Test_Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.01       820\n",
      "           1       0.82      1.00      0.90      3706\n",
      "\n",
      "    accuracy                           0.82      4526\n",
      "   macro avg       0.91      0.50      0.45      4526\n",
      "weighted avg       0.85      0.82      0.74      4526\n",
      "\n",
      "Train_Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.03      0.05      3281\n",
      "           1       0.82      1.00      0.90     14821\n",
      "\n",
      "    accuracy                           0.82     18102\n",
      "   macro avg       0.91      0.51      0.48     18102\n",
      "weighted avg       0.85      0.82      0.75     18102\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"NB MODEL\")\n",
    "eval(nb, X_train_tf_idf, X_test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25eb5009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy             score for tfidf : 0.8683022108599687\n",
      "\n",
      " precision-0          score for tfidf : 0.6469066182564343\n",
      "\n",
      " recall-0             score for tfidf : 0.6046917858996219\n",
      "\n",
      " f1-0                 score for tfidf : 0.6246485802894919\n",
      "\n",
      " precision-1          score for tfidf : 0.9137396731902699\n",
      "\n",
      " recall-1             score for tfidf : 0.9266583583810399\n",
      "\n",
      " f1-1                 score for tfidf : 0.9201277595505392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_scorer = {'accuracy': make_scorer(accuracy_score),\n",
    "                 'precision-0': make_scorer(precision_score, pos_label=0),\n",
    "                 'recall-0': make_scorer(recall_score, pos_label=0),\n",
    "                 'f1-0': make_scorer(f1_score, pos_label=0),\n",
    "                 'precision-1': make_scorer(precision_score, pos_label=1),\n",
    "                 'recall-1': make_scorer(recall_score, pos_label=1),\n",
    "                 'f1-1': make_scorer(f1_score, pos_label=1)\n",
    "                 }\n",
    "\n",
    "for i, j in custom_scorer.items():\n",
    "    model = BernoulliNB()\n",
    "    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n",
    "    if i == \"recall-1\":\n",
    "        nb_tfidf_rec = scores\n",
    "    elif i == \"f1-1\":\n",
    "        nb_tfidf_f1 = scores\n",
    "    print(f\" {i:20} score for tfidf : {scores}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c090050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log and nb are our 2 classifiers \n",
    "#lets try combining it\n",
    "\n",
    "preds1 = log.predict(X_test_tf_idf)\n",
    "preds2 = nb.predict(X_test_tf_idf)\n",
    "\n",
    "\n",
    "test_preds1 = log.predict(df_test_tf_idf)\n",
    "test_preds2 = nb.predict(df_test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ef214228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a new dataset for training our final model by stacking the predictions on the validation data\n",
    "train_stack = np.column_stack((preds1,preds2,preds3))\n",
    "\n",
    "#making the final test set for our final model by stacking the predictions on the test data\n",
    "test_stack = np.column_stack((test_preds1,test_preds2,test_preds3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "407272c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = RandomForestClassifier()\n",
    "\n",
    "#training the final model on the stacked predictions\n",
    "final_model.fit(train_stack,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ff763eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_model.predict(test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a1f4377f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6448\\3718425217.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcustom_scorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_tf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"recall-1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mfinal_model_tfidf_rec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    510\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    268\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    269\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    451\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "custom_scorer = {'accuracy': make_scorer(accuracy_score),\n",
    "                 'precision-0': make_scorer(precision_score, pos_label=0),\n",
    "                 'recall-0': make_scorer(recall_score, pos_label=0),\n",
    "                 'f1-0': make_scorer(f1_score, pos_label=0),\n",
    "                 'precision-1': make_scorer(precision_score, pos_label=1),\n",
    "                 'recall-1': make_scorer(recall_score, pos_label=1),\n",
    "                 'f1-1': make_scorer(f1_score, pos_label=1)\n",
    "                 }\n",
    "\n",
    "for i, j in custom_scorer.items():\n",
    "    model = RandomForestClassifier()\n",
    "    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n",
    "    if i == \"recall-1\":\n",
    "        final_model_tfidf_rec = scores\n",
    "    elif i == \"f1-1\":\n",
    "        final_model_tfidf_f1 = scores\n",
    "    print(f\" {i:20} score for tfidf : {scores}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e06c7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9065296652303613\n",
      "- MCC: 0.6787334777769296\n",
      "- F1 score: 0.9055174837092336\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8875386654882899\n",
      "- MCC: 0.6002976703972988\n",
      "- F1 score: 0.8838973216096795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define estimators\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "estimator_list = [\n",
    "    ('log',log),\n",
    "    ('nb',nb) ]\n",
    "\n",
    "# Build stack model\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Train stacked model\n",
    "stack_model.fit(X_train_tf_idf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = stack_model.predict(X_train_tf_idf)\n",
    "y_test_pred = stack_model.predict(X_test_tf_idf)\n",
    "\n",
    "# Training set model performance\n",
    "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "\n",
    "# Test set model performance\n",
    "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
    "print('- MCC: %s' % stack_model_train_mcc)\n",
    "print('- F1 score: %s' % stack_model_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
    "print('- MCC: %s' % stack_model_test_mcc)\n",
    "print('- F1 score: %s' % stack_model_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91642edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
